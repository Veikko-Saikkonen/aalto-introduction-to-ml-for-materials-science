{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veikko-Saikkonen/aalto-introduction-to-ml-for-materials-science/blob/main/Assignment_4_Tree_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6rc4C4LueRy"
      },
      "source": [
        "# Assignment 4: Tree methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeehoSHsueR1"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The purpose of this exercise is to test tree methods for regression and classification tasks.\n",
        "\n",
        "The assignment consists of four exercises:\n",
        "\n",
        "1. Building and visualizing a decision tree for regression of friction coefficients of different materials. (Essential content)\n",
        "2. Building a random forest for regression of friction coefficients of different materials. (Essential Content)\n",
        "3. Building a random forest for classification of the dimensionality of samples based on their X-ray diffraction (XRD) spectra. (Partly essential, partly optional content)\n",
        "4. Building a gradient boosted forest for classification of the dimensionality of samples based on their XRD spectra. (Optional content)\n",
        "\n",
        "The first exercise aims for introducing how tree-based decision making works and the second for introducing how the stability of the model is improved by building a forest of trees. In the last two exercises, two tree-based methods are compared in a cross-validation workflow that is close to real-life modelling workflow.\n",
        "\n",
        "--\n",
        "\n",
        "The friction coefficient data utilized in this exercise is originally published in (the article can be downloaded with Aalto VPN or in Aalto network):\n",
        "\n",
        "Bucholz, E.W., Kong, C.S., Marchman, K.R. et al. Data-Driven Model for Estimation of Friction Coefficient Via Informatics Methods. Tribol Lett 47, 211–221 (2012). https://doi.org/10.1007/s11249-012-9975-y\n",
        "\n",
        "The XRD data utilized in this exercise is originally published in:\n",
        "\n",
        "Oviedo, F., Ren, Z., Sun, S. et al. Fast and interpretable classification of small X-ray diffraction datasets using data augmentation and deep neural networks. npj Comput Mater 5, 60 (2019). https://doi.org/10.1038/s41524-019-0196-x\n",
        "\n",
        "The exercise structure is inspired by MIT PVLab's (Tonio Buonassisi's group) course exercises: https://github.com/PV-Lab/2s986_class (accessed 9/24/2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YXQ25hKueR3"
      },
      "source": [
        "## Initial operations\n",
        "\n",
        "Let's start by loading basic Python packages, installing and loading the X dataset, defining a data loader for XRD, and defining convenient plotting functions.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkzFoIzYueR5"
      },
      "outputs": [],
      "source": [
        "import time  \n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.signal import find_peaks_cwt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mT3at5RueR7"
      },
      "source": [
        "## 1. Decision tree for regression (essential content)\n",
        "\n",
        "### 1a. Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBlqW0efueR7"
      },
      "source": [
        "Let's start by initializing the data. Run the hidden cell below with a data loader defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zy933SSCueR8"
      },
      "outputs": [],
      "source": [
        "#@title Define a data loader.\n",
        "def fric_data_loader():\n",
        "    fric_data = pd.DataFrame([['MgO','Periclase',0.425,5.5,2,0.72,2.778,67.833,-1.747,-23.904,2.106,2.106,3098,1.31,3.44,2.13,3.6,40.304],\n",
        "    ['SiO2','Quartz',0.449,7,4,0.4,10,44.728,-1.474,-52.773,1.5,1.61,1995,1.9,3.44,1.54,2.648,60.084],\n",
        "    ['Al2O3','Corundum',0.4,9,3,0.54,5.556,56.709,-1.216,-28.334,1.327,1.855,2327,1.61,3.44,1.83,3.99,101.961],\n",
        "    ['ZnO','Zincite',0.7,4,2,0.74,2.703,55.113,-1.642,-23.885,1.796,1.981,2247,1.65,3.44,1.79,5.6,81.38],\n",
        "    ['CuO','Tenorite',0.4,3.5,2,0.77,2.597,44.728,-1.365,-20.199,1.277,1.948,1500,1.9,3.44,1.54,6.31,79.545],\n",
        "    ['FeO','Wustite',0.6,5,2,0.55,3.636,47.692,-1.747,-23.350,2.155,2.155,1650,1.83,3.44,1.61,6,71.844],\n",
        "    ['MoO3','Molybdite',0.235,3.5,6,0.69,8.696,33.608,-1.392,-61.521,2.102,1.956,1075,2.16,3.44,1.28,4.7,143.96],\n",
        "    ['NiO','Bunsenite',0.5,5.5,2,0.69,2.899,44.302,-1.747,-24.150,2.084,2.084,2230,1.91,3.44,1.53,6.72,74.693],\n",
        "    ['V2O5','Shcherbinaite',0.31,3.25,5,0.79,6.329,55.914,-1.486,-58.475,2.303,1.831,954,1.63,3.44,1.81,3.35,181.88],\n",
        "    ['TiO2','Rutile',0.45,6.2,4,0.86,4.651,59.445,-1.600,-47.076,1.983,1.958,2116,1.54,3.44,1.9,4.17,79.866],\n",
        "    ['SnO2','Cassiterite',0.5,6.5,4,0.69,5.797,42.166,-1.600,-44.890,2.057,2.054,1903,1.96,3.44,1.48,6.85,150.709],\n",
        "    ['ZrO2','Baddeleyite',0.5,6.5,4,0.72,5.556,67.144,-1.660,-43.753,1.29,2.187,2983,1.33,3.44,2.11,5.68,123.223],\n",
        "    ['Ag2S','Acanthite',0.101,2.3,1,1.15,0.87,10.024,-1.576,-8.921,2.072,2.546,1098,1.93,2.58,0.65,7.23,247.801],\n",
        "    ['WS2','Tungstenite',0.043,2.5,4,0.6,6.667,1.203,-1.283,-30.666,3.124,2.411,1523,2.36,2.58,0.22,7.6,247.97],\n",
        "    ['PbS','Galena',0.202,2.5,2,1.19,1.681,1.55,-1.747,-16.957,2.968,2.968,1386,2.33,2.58,0.25,7.6,239.3],\n",
        "    ['Cu2S','Chalcocite',0.315,2.8,1,0.77,1.299,10.917,-1.567,-9.791,1.427,2.306,1402,1.9,2.58,0.68,5.6,159.157],\n",
        "    ['MoS2','Molybdenite',0.22,1.3,4,0.69,5.797,4.314,-1.283,-30.486,2.98,2.425,1458,2.16,2.58,0.42,5.06,160.09],\n",
        "    ['FeS2','Pyrite',0.2,6.3,2,0.55,3.636,13.118,-0.791,-10.070,1.464,2.264,1444,1.83,2.58,0.75,5.02,119.975],\n",
        "    ['ZnS','Sphalerite',0.527,3.8,2,0.74,2.703,19.445,-1.637,-20.141,1.913,2.342,1973,1.65,2.58,0.93,4.04,97.44],\n",
        "    ['Sb2S3','Stibnite',0.3,2,3,0.76,3.947,6.782,-1.551,-25.842,1.915,2.594,823,2.05,2.58,0.53,4.562,339.715],\n",
        "    ['CdS','Greenockite',0.37,3.3,2,0.95,2.105,17.965,-1.642,-18.681,2.599,2.532,1753,1.69,2.58,0.89,4.826,144.476],\n",
        "    ['NiS','Millerite',0.24,3.3,2,0.69,2.899,10.616,-1.626,-20.321,1.635,2.306,1249,1.91,2.58,0.67,5.5,90.758],\n",
        "    ['MoSe2','Drysdallite',0.06,2,4,0.69,5.797,3.731,-1.283,-29.257,3.118,2.527,1473,2.16,2.55,0.39,6.9,253.88],\n",
        "    ['ZnSe','Stilleite',0.49,5,2,0.74,2.703,18.331,-1.637,-19.222,2.004,2.454,1790,1.65,2.55,0.9,5.65,144.34],\n",
        "    ['GaSe','P63/mmc',0.23,2,2,0.62,3.226,12.794,-1.039,-12.054,3.184,2.484,1233,1.81,2.55,0.74,5.03,148.68],\n",
        "    ['CoSe','Freboldite',0.28,2.75,2,0.65,3.077,10.616,-1.706,-19.832,1.325,2.479,1328,1.88,2.55,0.67,7.65,137.89],\n",
        "    ['Cu2Se','Berzelianite',0.49,2.7,1,0.77,1.299,10.024,-1.554,-8.855,1.46,2.529,1386,1.9,2.55,0.65,6.84,206.05],\n",
        "    ['PbSe','Clausthalite',0.19,2.75,2,1.19,1.681,1.203,-1.747,-16.375,3.074,3.074,1351,2.33,2.55,0.22,8.1,286.2],\n",
        "    ['CdTe','Zinc blende',0.718,3,2,0.95,2.105,4.115,-1.637,-16.810,2.291,2.806,1365,1.69,2.1,0.41,6.2,240.01],\n",
        "    ['NiTe','Imgreite',0.28,4,2,0.69,2.899,0.898,-1.706,-18.566,1.339,2.648,1133,1.91,2.1,0.19,8.38,186.293],\n",
        "    ['GaAs','Zinc blende',0.405,4.5,3,0.62,4.839,3.365,-2.455,-43.357,1.999,2.448,1511,1.81,2.18,0.37,5.318,144.645],\n",
        "    ['CaF2','Fluorite',0.372,4,2,1,2,89.14,-0.839,-10.224,1.366,2.366,1691,1,3.98,2.98,3.18,78.075],\n",
        "    ['BaF2','Frankdicksonite',0.392,2.5,2,1.35,1.481,90.81,-0.839,-9.009,1.55,2.685,1641,0.89,3.98,3.09,4.893,175.324],\n",
        "    ['MgF2','Sellaite',0.429,5,2,0.72,2.778,83.174,-0.801,-11.583,1.981,1.992,1536,1.31,3.98,2.67,3.148,62.302],\n",
        "    ['NaCl','Halite',0.303,2,1,1.02,0.98,71.155,-0.873,-4.461,2.82,2.82,1073.7,0.93,3.16,2.23,2.17,58.443],\n",
        "    ['KCl','Sylvite',0.319,2,1,1.38,0.725,74.561,-0.873,-3.999,3.146,3.146,1044,0.82,3.16,2.34,1.988,74.551],\n",
        "    ['KBr','Rock salt',0.379,1.5,1,1.38,0.725,68.174,-0.873,-3.813,3.3,3.3,1007,0.82,2.96,2.14,2.74,119.002],\n",
        "    ['YPO4','Xenotime',0.357,4.5,3,0.9,3.333,52.884,-2.804,-51.689,2.243,2.345,2268,1.71,3.44,1.74,4.8,183.877]],\n",
        "    columns = ['Chemical formula','Structure/phase','Friction coefficient','Mohs hardness','Formal cation charge',\n",
        "               'Cation radius (Å)','Ionic potential','Percent ionicity (%)','Madelung constant',\n",
        "               'Electrostatic potential (eV/atom)','Interplanar spacing (Å)','R ij distance (Å)',\n",
        "               'Melting temperature (K)','Electronegativity of cation','Electronegativity of anion','EN difference','Density (g/cc)',\n",
        "               'Molar weight (g/mol)'])\n",
        "    \n",
        "    fric = fric_data.loc[:, 'Friction coefficient'].values\n",
        "    \n",
        "    features_to_pick = [12,13,16]\n",
        "    x = fric_data.iloc[:,features_to_pick].values\n",
        "    \n",
        "    columns_to_pick = [0,1,2] + features_to_pick\n",
        "    \n",
        "    fric_data = fric_data.iloc[:,columns_to_pick]\n",
        "    \n",
        "    return fric_data, fric, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbshbNp2ueR-"
      },
      "source": [
        "Let's load the data and familiarize ourselves with it. Fill in the gaps below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR4tkYgnueR_"
      },
      "outputs": [],
      "source": [
        "# Load the friction coefficient data.\n",
        "raw_fric_data, fric, x = fric_data_loader()\n",
        "\n",
        "# Print the first 10 rows of the raw_fric_data dataframe to get a general look at the data.\n",
        "\n",
        "# Print out the shape of the dataframe and numpy arrays fric and x:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKna0vQQueSA"
      },
      "source": [
        "There are 38 samples in the dataframe. In this exercise, we aim for precting friction coefficients (\"y data\") as a function of other target properties. You can also see that there are non-numeric variables in the dataframe. For simplicity, we use only numeric variables as the input data for the models (\"x data\").\n",
        "\n",
        "For convenience, the friction coefficients have already been saved into numpy array \"fric\" and the other numeric variables into numpy array \"x\". Fill in the gaps below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQg4ayMaueSC"
      },
      "outputs": [],
      "source": [
        "# Print variables fric and x.\n",
        "\n",
        "# Print the number of samples in the friction coefficient data.\n",
        "n_samples = None\n",
        "print(n_samples)\n",
        "\n",
        "# Print the number of variables (features) in the x fata.\n",
        "n_dimensions = None\n",
        "print(n_dimensions)\n",
        "\n",
        "# Let's save and print the variable (feature) names in the x data\n",
        "# (these need to be exactly same than column names in the dataframe).\n",
        "x_names = None\n",
        "print(x_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check previous section\n",
        "assert x_names[0] == 'Melting temperature (K)', 'x_names element 0 is not correctly defined, please check.'\n",
        "assert x_names[1] == 'Electronegativity of cation', 'x_names element 1 is not correctly defined, please check.'\n",
        "assert x_names[2] == 'Density (g/cc)', 'x_names element 2 is not correctly defined, please check.'\n",
        "print(\"Yay ! All tests successsfully passed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xGVHtj21kjff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdx4-DM6ueSD"
      },
      "source": [
        "### 1b. Decision tree regression for friction coefficient data\n",
        "\n",
        "Let's fit a decision tree regressor model to predict friction coefficient based on materials properties in array \"x\".\n",
        "\n",
        "Start by dividing the data into train and test sets. Fill in the gaps below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLzm0WXueSE"
      },
      "outputs": [],
      "source": [
        "# Choose the number of samples in the train data.\n",
        "n_train = None\n",
        "\n",
        "# Remember how the data was shuffled and then divided\n",
        "# into train and test in the exercise session last week?\n",
        "# Let's do the same here (no need to do changes below).\n",
        "\n",
        "# The rest of the data goes to test set.\n",
        "n_test = n_samples - n_train\n",
        "\n",
        "# shuffle the data\n",
        "c = list(zip(x, fric))\n",
        "random.shuffle(c)\n",
        "x, frict = zip(*c)\n",
        "x = np.array(x)\n",
        "fric = np.array(fric)\n",
        "\n",
        "# split data in training and test\n",
        "# take first n_train samples for training\n",
        "x_train  = x[0:n_train] \n",
        "fric_train = fric[0:n_train]\n",
        "\n",
        "# take the next n_test data for testing\n",
        "x_test = x[n_train:n_train + n_test]\n",
        "fric_test = fric[n_train:n_train + n_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4noGFpOueSE"
      },
      "source": [
        "Let's create and fit a tree model with train data, and then use it for predicting test data. Finally, plot the regression result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OWXGifDueSF"
      },
      "outputs": [],
      "source": [
        "# Let's import DecisionTreeRegressors.\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Let's create a tree object.\n",
        "model = DecisionTreeRegressor()\n",
        "# Let's it it with train data.\n",
        "model.fit(x_train, fric_train)\n",
        "\n",
        "# Let's predict test data with the model.\n",
        "fric_pred = model.predict(x_test)\n",
        "\n",
        "# Let's plot the prediction result.\n",
        "plt.figure(1)\n",
        "# Let's plot predictions as the function of real values:\n",
        "plt.plot(fric_test, fric_pred, 'x', color = 'b', label = 'Test dataset')\n",
        "plt.ylabel('Predicted friction coeffiecient')\n",
        "plt.xlabel('True friction coeffiecient')\n",
        "# A perfect fit would match 1-to-1 and thus follow this line:\n",
        "plt.plot([0, 1], [0, 1], 'k--', label = 'Perfect fit')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cVQJigYueSG"
      },
      "source": [
        "Not that impressive, right? How does the tree make it decisions? Let's visualize the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOidLPRvueSG"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "fig = plt.figure()\n",
        "# Play around with this setting if the tree layout is not nice for your computer screen.\n",
        "fig.set_size_inches(25, 15)\n",
        "plot_tree(model, filled = True, feature_names = x_names, fontsize = 8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBKiFIztueSI"
      },
      "source": [
        "In the graph above, if the decision criterion in a certain node is fulfilled (\"True\") for a certain sample, the sample goes down the left-side path. If the criterion is not fulfilled (\"False\"), the sample goes down the right-side path.\n",
        "\n",
        "The color of the boxes has a meaning, too: it represents the high (dark color) or low (pale color) friction coefficient values flowing through the node.\n",
        "\n",
        "Check how many samples there are assigned to the final leafs (nodes with no children) of the tree. Then, calculate how many train data samples have been assigned to the final leafs in the tree in total."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpMtBIlueSI"
      },
      "source": [
        "Now, what happens if we check the predictions on the _train_ data from this tree? Fill in the gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnzPpk11ueSI"
      },
      "outputs": [],
      "source": [
        "# Predict train data with the model.\n",
        "fric_pred_on_train = None\n",
        "\n",
        "# Let's plot the prediction result.\n",
        "plt.figure()\n",
        "# Let's plot test set predictions as the fucntion of real values:\n",
        "\n",
        "# Plot here the train set predictions with red color:\n",
        "\n",
        "\n",
        "plt.ylabel('Predicted friction coeffiecient')\n",
        "plt.xlabel('True friction coeffiecient')\n",
        "# A perfect fit would match 1-to-1 and thus follow this line:\n",
        "plt.plot([0, 1], [0, 1], 'k--', label = 'Perfect fit')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEWDN0ZkueSJ"
      },
      "source": [
        "Perfect predictions for the train data! Seems rather surprising, taking into account that the test set predictions are poor.\n",
        "\n",
        "But if you think about the tree illustration before, it is not such a surprise. Each leaf node was assigned to exactly one train datapoint. Thus, the train datapoint predictions always lead to these nodes and result in perfect values.\n",
        "\n",
        "In the above graph, compare test datapoints carefully with train datapoints. You can zoom in in the above graph with 'plt.ylim([low, high])' and 'plt.xlim([low, high])' if you like.\n",
        "\n",
        "The predictions of each test datapoint are exactly equal with the friction coefficient value of a corresponding train set datapoint! This means that in a single tree, the test datapoints cannot get any other values than those that were found from the train dataset. You can confirm this effect also by training a tree with a small train dataset (e.g. n_train = 3) - the test set prediction indeed get assigned discrete values from the train dataset friction coefficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yt-O0KsueSJ"
      },
      "source": [
        "### 1c. Hyperparameters of a decision tree -based model\n",
        "\n",
        "Decision tree -based models have a lot of hyperparameters. Here, we test one of them: maximum depth of the tree (_max_depth_ in scikit-learn).\n",
        "\n",
        "Calculate from the tree visualization above how many layers there were in the tree. Then try a lower value: Create a tree object with the chosen maximum depth given as a parameter, fit it, predict results for the test and train dataset, plot them both, and plot the tree visualization again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkn-UUbnueSJ"
      },
      "outputs": [],
      "source": [
        "# Let's create a tree object with max_depth of 3 .\n",
        "model = None\n",
        "# Fit it it with train data.\n",
        "\n",
        "# Predict test data with the model.\n",
        "\n",
        "# Predict train data with the model.\n",
        "\n",
        "# Plot the prediction result of train and test datasets.\n",
        "\n",
        "# Plot the tree visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERVFYh2cueSK"
      },
      "source": [
        "Now the train dataset predictions are not exact anymore. This is because there are multiple samples grouped into the leaf nodes, and they all get assigned the same value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kqo_QHueSL"
      },
      "source": [
        "## 2. Random forest regression (essential content)\n",
        "\n",
        "### 2a. Random forest regression for friction coefficients data\n",
        "\n",
        "Decision trees are visually interpretable models, but not very stable. In random forest regression, a forest of trees is trained to do the prediction and the final prediction is averaged among each tree. Let's fit a random forest regression model to predict the friction coefficient based on the descriptors in array 'x'. Run the cell below. Fill in the gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAQIyR74ueSL"
      },
      "outputs": [],
      "source": [
        "# Let's import RandomForestRegressor from sklearn.ensemble.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Let's create RF classifier model:\n",
        "model = RandomForestRegressor()\n",
        "# Fit the model with train data:\n",
        "\n",
        "# Predict test data with the model.\n",
        "fric_pred = None\n",
        "\n",
        "# Predict train data with the model.\n",
        "fric_pred_on_train = None\n",
        "\n",
        "# Let's plot the prediction result.\n",
        "plt.figure(1)\n",
        "# Plot test set predictions as the function of real values:\n",
        "\n",
        "plt.ylabel('Predicted friction coeffiecient')\n",
        "plt.xlabel('True friction coeffiecient')\n",
        "# A perfect fit would match 1-to-1 and thus follow this line:\n",
        "plt.plot([0, 1], [0, 1], 'k--', label = 'Perfect fit')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU0ge7dNueSL"
      },
      "source": [
        "Looks somewhat better already with the default settings! How many trees there are in the random forest regression model above? Hint: check the documentation ( https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuRPLzkjueSM"
      },
      "source": [
        "Random forest regression methods are difficult to visualize with the tree illustration because there are multiple trees in the forest. A common method to analyze the decision making of the model is feature importance ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmlCS_99ueSM"
      },
      "outputs": [],
      "source": [
        "# Feature importance ranking of x features:\n",
        "print(model.feature_importances_)\n",
        "print(x_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-ETQVmueSM"
      },
      "source": [
        "The more important the feature is, the higher the feature importance value. Check from the documentation what is the range of feature importance values. How important is the feature with the highest feature importance ranking compared to the other features in the train data? Or are they all equally important based on this ranking?\n",
        "\n",
        "Note that the feature importance values tell about how important the feature is on average - it does not tell if high or low values of that feature lead to high values of the output.\n",
        "\n",
        "--\n",
        "\n",
        "In this exercise we used only three features for the regression model (for easier visualization) but you may remember from the class that the source article listed many more features. They are all loaded into this notebook, so nothing prevents you from training a random forest regression model on all the features and comparing to the model in the source article. But for now it is time to move on to XRD data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfGi71dueSN"
      },
      "source": [
        "## 3. Random forest classification of XRD data (essential and optional content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw1kjUQ6ueSN"
      },
      "source": [
        "Crystalline thin film samples may adopt different dimensionalities depending on their composition and sometimes even within fixed composition depending on their preparation methods. The dimensionality of the sample can be determined from its XRD spectrum but the analysis is time-consuming, especially if one needs to analyze large datasets. The purpose of this exercise is to divide the data into train and test sets, train a random forest classifier to automatically classify the XRD spectra based on their dimensionality using the train dataset, and finally test it on test dataset. This model could then be used as a pre-screening step for analyzing new XRD data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACwXnrtcueSO"
      },
      "source": [
        "### 3a. Preprocessing of XRD data (Essential content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngfSOdkRueSO"
      },
      "source": [
        "Let's start by initializing the data. Run the hidden cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn0oWWb9ueSO"
      },
      "outputs": [],
      "source": [
        "#@title Define a raw data loader for XRD data.\n",
        "def load_raw_data(from_file = False):\n",
        "    \n",
        "    if from_file:\n",
        "        rawdata = pd.read_csv('./data/exp_d.csv',header=None)\n",
        "    else:\n",
        "        url = 'https://raw.githubusercontent.com/PV-Lab/2s986_class/master/Week7/data/exp_d.csv'\n",
        "        rawdata = pd.read_csv(url, header = None)\n",
        "        \n",
        "    return rawdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLOPldWnueSP"
      },
      "source": [
        "Let's define a data loader and plotting function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SBFGs1gueSP"
      },
      "outputs": [],
      "source": [
        "def data_loader():\n",
        "    '''\n",
        "    Data loader specifically for this exercise.\n",
        "    '''\n",
        "    \n",
        "    # Load raw data.\n",
        "    rawdata = load_raw_data()\n",
        "    # Format the raw data as a numpy array.\n",
        "    rawdata = rawdata.values\n",
        "    rawdata[0,:] = [int(x[0]) for x in list(rawdata[0,:])]\n",
        "    rawdata[1::,:] = rawdata[1::,:].astype(float)\n",
        "    rawdata = np.delete(rawdata, 1, axis=0)\n",
        "    # Save back to a dataframe.\n",
        "    rawdata = pd.DataFrame(data = rawdata)\n",
        "    rawdata = rawdata.rename(index={0: 'Dimensionality'})\n",
        "    \n",
        "    rawdata = rawdata.transpose()\n",
        "    \n",
        "    # XRD theta angles from the XRD spectra\n",
        "    # (every second column in the dataframe but dropping the first row with the dimensionality labels):\n",
        "    xrd_angles = rawdata.iloc[::2,1::].values\n",
        "    \n",
        "    # XRD intensities (same idea than for picking theta angles, but starting from the second column):\n",
        "    xrd_intensities = rawdata.iloc[1::2, 1::].values\n",
        "\n",
        "    # Class labels i.e. dimensionalities:\n",
        "    dimensionalities = np.ravel(rawdata.iloc[::2,0].values).astype('int')\n",
        "    \n",
        "    return rawdata, dimensionalities, xrd_angles, xrd_intensities\n",
        "\n",
        "def plot_xrd(theta_angles, intensities, colors = None, legend = None):\n",
        "    '''\n",
        "    Convenience function for plotting XRD and setting the axis labels.\n",
        "    '''\n",
        "    \n",
        "    plt.plot(theta_angles.T, intensities.T, color = colors)\n",
        "    \n",
        "    plt.xlabel(r'2$\\theta$ ($^{\\circ}$)')\n",
        "    plt.ylabel('Intensity (a.u.)')\n",
        "    \n",
        "    if legend is not None:\n",
        "        plt.legend(legend)\n",
        "    \n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMulm1ozueSQ"
      },
      "source": [
        "Let's load the data and familiarize ourselves with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3IsAEcTueSQ"
      },
      "outputs": [],
      "source": [
        "# Load the XRD spectra\n",
        "rawdata, dimensionalities, xrd_angles, xrd_intensities = data_loader()\n",
        "\n",
        "# Print out the shapes of dimensionalities, XRD angles, and XRD intensities arrays:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICeWrHg2ueSR"
      },
      "source": [
        "This dataset involves 73 samples with spectra with 3248 theta angles. This time, there is a theta vector for each sample - this is because the XRD spectra have been collected qith differing resolutions and/or ranges of theta.\n",
        "\n",
        "In this dataset, we do not know the compositions of the samples. This is because the dataset has anonymized by the authors of the article it was published in (this is sometimes done due to copyright issues).\n",
        "\n",
        "How do the XRD data look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgzkF3MzueSR"
      },
      "outputs": [],
      "source": [
        "# How does XRD data look? Save the XRD intensities of samples 0 and 20 into this variable and print them.\n",
        "example_intensities = None\n",
        "\n",
        "# Then save and print the theta angles of the example samples.\n",
        "example_angles = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5V1rc0WueSS"
      },
      "source": [
        "Note that there are NaN values at the end of the vectors. The arrays have been padded with NaNs due to different lengths of the spectra.\n",
        "\n",
        "Let's plot the example XRD spectra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imUiap_KueSS"
      },
      "outputs": [],
      "source": [
        "# Let's plot the XRD intensities of the example samples as a function of XRD angle theta.\n",
        "# For convenience, we use the function \"plot_xrd\" that has been defined above.\n",
        "plt.figure(1)\n",
        "plot_xrd(example_angles, example_intensities, legend = ['Sample 0', 'Sample 20'])\n",
        "plt.show()\n",
        "\n",
        "# Save the dimensionalities of the example samples 0 and 20 into this variable and print them.\n",
        "example_dimensionalities = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1OGBiBiueSS"
      },
      "source": [
        "Well, at least these two samples with differing dimensionalities look quite different. We notice also two things:\n",
        "- The intensity scale of the XRD spectra varies. You may notice from Tutorial 2 that this is due to the sample properties but also due to sample orientation and measurement device. The data needs to be normalized.\n",
        "- We also notice that the spectra contain noise and background signal (especially visible in sample 0) that makes it more difficult to classify the data. We should remove background noise by filtering.\n",
        "\n",
        "Let's start by the filtering task. Let's define a function to remove background noise (initially, this function is from the source article - you don't need to understand the steps), then apply it to the data.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npITPDOXueST"
      },
      "outputs": [],
      "source": [
        "def exp_data_processing(data,minn,maxn,window):\n",
        "    data = data.T\n",
        "    (len1,w1) = np.shape(data)\n",
        "    nexp1 =np.zeros([maxn-minn,w1])\n",
        "    for i in range(w1):\n",
        "        #savgol_filter to smooth the data\n",
        "         new1 = savgol_filter(data[minn:maxn,i], 31, 3)\n",
        "         #peak finding\n",
        "         zf= find_peaks_cwt(new1, np.arange(10,15), noise_perc=0.01)\n",
        "         #background substraction\n",
        "         for j in range(len(zf)-1):\n",
        "             zf_start= np.maximum(0,zf[j+1]-window//2)\n",
        "             zf_end = np.minimum(zf[j+1]+window//2,maxn)\n",
        "             peak = new1[zf_start:zf_end]\n",
        "\n",
        "             ##arbitrarily remove 1/4 data\n",
        "             npeak = np.maximum(0,peak-max(np.partition(peak,window//5 )[0:window//5]))\n",
        "             nexp1[zf_start:zf_end,i]= npeak\n",
        "                \n",
        "    nexp1 = nexp1.T\n",
        "    \n",
        "    return nexp1\n",
        "\n",
        "#window size for experimental data\n",
        "window =15\n",
        "#theta range\n",
        "exp_min = 0\n",
        "exp_max = 1350\n",
        "\n",
        "# Clean up the data by filtering out the background.\n",
        "filtered_intensities = exp_data_processing(xrd_intensities,exp_min,exp_max,window)\n",
        "# The function also cut all the spectra to the same length so let's update theta angles, too.\n",
        "filtered_angles = xrd_angles[:, exp_min:exp_max]\n",
        "\n",
        "# Update the example spectra.\n",
        "example_intensities = filtered_intensities[[0,20], :]\n",
        "example_angles = filtered_angles[[0,20], :]\n",
        "\n",
        "# Plot example sample 0 before and after the clean-up.\n",
        "plt.figure(3)\n",
        "plot_xrd(np.stack((example_angles[0,:], example_angles[0,:]), axis=0),\n",
        "         np.stack((xrd_intensities[0, exp_min:exp_max], filtered_intensities[0,:]), axis = 0),\n",
        "         legend = ['Sample 0 before clean-up', 'Sample 0 after clean-up'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0lHu2uVueST"
      },
      "source": [
        "Looks better! Let's continue by normalizing the data. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N69PkEfxueSU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import minmax_scale\n",
        "norm_intensities = minmax_scale(filtered_intensities, axis = 1)\n",
        "\n",
        "example_intensities = norm_intensities[[0,20],:]\n",
        "\n",
        "plt.figure(2)\n",
        "plot_xrd(example_angles, example_intensities, legend = ['Sample 0', 'Sample 20'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrRh7F-ueSU"
      },
      "source": [
        "The data looks good now. Let's save the results to the final variables. Use these variables for XRD data from now on.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SjE5ozrueSV"
      },
      "outputs": [],
      "source": [
        "cleaned_intensities = norm_intensities\n",
        "cleaned_angles = filtered_angles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ6zISFjueSV"
      },
      "source": [
        "### 3b. Fitting random forest classifier model (Essential content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp--PLwPueSW"
      },
      "source": [
        "Normally, you would perform hyperparameter optimization at this stage. This would be done by dividing the data randomly into train and test sets, and performing hyperparameter optimization using only the train data. Here, this step is omitted for the sake of time and we use a list of pre-defined hyperparameters below for the random forest classifier models. You can read more about hyperparameters of random forest models for example from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZyP8Tm-ueSX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This code could be used as a starting point for hyperparameter optimization in your own projects:\n",
        "\n",
        "random_grid = {'n_estimators': array([10, 25, 50, 75, 100, 500, 1000]),\n",
        " 'max_depth': [1, 5, 10, 20, 50, 75, 100],\n",
        " 'min_samples_split': [1, 2, 5, 10],\n",
        " 'min_samples_leaf': [1, 2, 3, 4],\n",
        " 'bootstrap': [True, False],\n",
        " 'criterion': ['gini', 'entropy']}\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "rf_random_cv = RandomizedSearchCV(estimator = model,\n",
        "                               param_distributions = random_grid,\n",
        "                               n_iter = 30, cv = 5,\n",
        "                               verbose=2,\n",
        "                               random_state=0)\n",
        "rf_random_cv.fit(x_train, y_train)\n",
        "'''\n",
        "\n",
        "# Number of trees in the forest.\n",
        "n_estimators = 100\n",
        "\n",
        "# The maximum number of features considered for splitting a node.\n",
        "max_features = 'sqrt'\n",
        "\n",
        "# Maximum depth of the tree.\n",
        "max_depth = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R6Lj1LrueSX"
      },
      "source": [
        "Now, we divide the XRD data randomly into 5 folds of equal size. One fold is set as the test set, the rest are set as the train set.\n",
        "\n",
        "In the first exercise and in the tutorial last week, we shuffled the data by ourselves, chose the train and test set sizes, and divided the data into two vectors. This process can also be done automatically in scikit-learn for example with the steps shown below.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Ojk4RqueSY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Number of folds. Means that the whole dataset is divided into n_fold parts of equal sizes.\n",
        "n_fold = 5\n",
        "\n",
        "# Create the KFold object.\n",
        "k_fold = KFold(n_splits=n_fold, shuffle=True,random_state=0)\n",
        "\n",
        "# Train and test indices for each fold:\n",
        "train_test_idx_in_folds = {k: (train, test) for k, (train, test) in enumerate(k_fold.split(dimensionalities))}\n",
        "\n",
        "# Let's print train set indices for each fold:\n",
        "for i in range(n_fold):\n",
        "    \n",
        "    print('Train set indices for fold ' + str(i) + ': ', train_test_idx_in_folds[i][0])\n",
        "    \n",
        "    print('Test set indices for fold ' + str(i) + ': ', train_test_idx_in_folds[i][1], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRu8DxtNueSY"
      },
      "source": [
        "You can notice that each test set index appears in the test set only in one fold, in other words, these folds do not overlap. There are also multiple other ways to define folds but this is typically a good starting point.\n",
        "\n",
        "Let's use fold 0 train and test set as our train and test set for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04vHvUUiueSZ"
      },
      "outputs": [],
      "source": [
        "# Choose indices of train and test sets from fold 0:\n",
        "fold = 0\n",
        "train_idx = None\n",
        "test_idx = None\n",
        "\n",
        "# Pick input data for train and test sets (i.e., cleaned XRD intensities):\n",
        "train_x = None\n",
        "test_x = None\n",
        "\n",
        "# Pick output data for train and test sets (i.e., sample dimensionalities):\n",
        "train_y = None\n",
        "test_y = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoLWZ5AhueSZ"
      },
      "source": [
        "Next, train a random forest _classification_ model for predicting the dimensionality of the XRD data based on the (cleaned-up) XRD intensities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkxiBXxuueSZ"
      },
      "outputs": [],
      "source": [
        "# Import RandomForestClassifier from sklearn.ensemble.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create RF classifier model with the hyperparameter settings you defined a bit earlier:\n",
        "model = None\n",
        "# Fit the model:\n",
        "\n",
        "# Predict test set dimensionalities and print them out. Compare to the true dimensionalities.\n",
        "pred_y = None\n",
        "\n",
        "print('Predictions: ', None, '\\nTrue values: ', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8MkAgUueSa"
      },
      "source": [
        "Now the random forest classifier is working but we still need to perform cross-validation to find out how well it performs.\n",
        "\n",
        "### 3c. Cross-validation for performance evaluation (Optional content)\n",
        "\n",
        "Turn the random forest classifier above into a function for easier use during cross-validation. Fill in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-SdAzucueSa"
      },
      "outputs": [],
      "source": [
        "def my_RF_classifier(train_x, train_y, test_x):\n",
        "    \n",
        "    # Create RF classifier model.\n",
        "    model = None\n",
        "    # Fit the model:\n",
        "    \n",
        "    # Predict test set dimensionalities:\n",
        "    pred_y = None\n",
        "    \n",
        "    return pred_y, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5co2oTsueSb"
      },
      "source": [
        "Now we are ready for the cross validation. Fill in the gaps below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fEmnXwKueSb"
      },
      "outputs": [],
      "source": [
        "# This variable will be filled in during the for loop with cross validation scores.\n",
        "accuracy = np.empty((n_fold,1)) \n",
        "# Let's time the for loop below.\n",
        "start_time = time.time()   \n",
        "\n",
        "for i in range(n_fold):\n",
        "    \n",
        "    # Define train and test sets for the current fold:\n",
        "    \n",
        "    # Use my_RF_classifier for fitting a random forest classifier and predicting the test set value with it.\n",
        "    pred_y, model = my_RF_classifier(train_x, train_y, test_x)\n",
        "    \n",
        "    # Accuracy core for predicting the sample dimensionalities.\n",
        "    accuracy[i] = accuracy_score(test_y, pred_y)  \n",
        "    print ('Accuracy in fold ' + str(i) + ': %.2f%%' % (100 * accuracy[i]))\n",
        "        \n",
        "print ('Cross validation took %fs!' % (time.time() - start_time) )\n",
        "print('Cross-validation results:')\n",
        "print('Folds: %i, mean accuracy: %.3f' % (len(accuracy), np.mean(np.abs(accuracy))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_bjA86pueSb"
      },
      "source": [
        "The random forest classifier was able to predict the dimensionality of the sample correct based on its XRD spectrum on average [mean accuracy]% of the tries during cross validation. This is good enough score for the random forest classifier to be useful in the actual use for pre-screening XRD data (but as always, you should also confirm the model predictions also by other means, for example by looking the spectra by yourself).\n",
        "\n",
        "Next, you could proceed with more detailed cross validation to understand cases when your model performs well and when it does not. There are multiple different metrics for evaluating the predictions, and it is useful to compare their results. More information about typical classification accuracy metrics is for example here: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
        "\n",
        "Finally, you could either take any of the fold models and deploy it (default option), or use the same settings to train another random forest classifier with the whole dataset to be deployed (not a default option but is sometimes used with small datasets).\n",
        "\n",
        "Look at the cross validation scores in each fold above. Some of them may perform clearly worse or better than the others. The variability depends on the random choice of the test and train dataset, and would likely be reduced with a larger source dataset. This could be done by collecting more experimental data. Another option, chosen by the authors of the source data article (link at the beginning of the exercise), is to further augment data to improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmp0X8PmueSc"
      },
      "source": [
        "## 4: Boosted tree classifier for XRD data (Optional content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6KH8GEKueSc"
      },
      "source": [
        "In the previous exercise you built a random forest regressor to predict the dimensionality of XRD data but is that the best possible type of model for the task? Typically, multiple different machine learning models (also different types of models) should be compared when building a regression or classification model because some models may perform better for a certain task.\n",
        "\n",
        "In this exercise, you will compare the performance of the random forest classifier to a gradient boosting classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFUnwy8ueSc"
      },
      "source": [
        "Normally, you would optimize the hyperparameters for the gradient boosting classifier. For the sake of time, this step is omitted. Use the following hyperparameters, instead. Run the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvRALKTNueSd"
      },
      "outputs": [],
      "source": [
        "# Number of trees in the forest.\n",
        "n_estimators = 100\n",
        "\n",
        "# The maximum number of features considered for splitting a node.\n",
        "max_features = 'sqrt'\n",
        "\n",
        "# Maximum depth of the tree.\n",
        "max_depth = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJuFJDAiueSd"
      },
      "source": [
        "Define a function that creates a gradient boosting classifier, fits it to the train data, and makes prediction on the test data. Fill in the gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yooK9cQoueSd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "def my_boosted_tree_classifier(train_x, train_y, test_x):\n",
        "    \n",
        "    # Create classifier model with the hyperparameters defined above:\n",
        "    model = None\n",
        "    # Fit the model:\n",
        "    \n",
        "    # Predict test set dimensionalities:\n",
        "    pred_y = None\n",
        "    \n",
        "    return pred_y, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3w3ss9nueSe"
      },
      "source": [
        "Repeat the cross validation in a same way than with the random forest classifier above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usGfFyY8ueSe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWEiKHn8ueSe"
      },
      "source": [
        "Which one of the two models performed better? Which one was faster?\n",
        "\n",
        "In on the tests you did here, the gradient boosting classifier likely performed slightly better than the random forest classifier but took a bit longer to train, so the gradient boosting classifier should be chosen for further use (for some choices of the train and test sets the two models may also perform similarly). \n",
        "\n",
        "Do you want to see how other types of models performed against the two tree-based models tested in this exercise? Check the source article for more comparisons."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "ml4mscourse",
      "language": "python",
      "name": "ml4mscourse"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}